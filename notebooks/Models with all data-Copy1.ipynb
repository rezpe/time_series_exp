{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98f22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b15708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stations = os.listdir(\"/home/sebas/data/air_qual_aemet/\")\n",
    "stations = pd.Series(stations).apply(lambda e: e.split(\".csv\")[0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364bff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32c73e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "all_dfs=np.array([])\n",
    "for station in stations:\n",
    "    df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "    all_dfs = np.concatenate([all_dfs,df[\"SPA.NO2\"].values])\n",
    "scaler.fit(all_dfs.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56ab293",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MinMaxScaler.transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h\u001b[38;5;241m>\u001b[39mhorizon:\n\u001b[1;32m     25\u001b[0m         tdf[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO2 - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m(tdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(h)\u001b[38;5;241m-\u001b[39mtdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrend_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 26\u001b[0m         tdf[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO2 - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tdf\u001b[38;5;241m=\u001b[39mtdf\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     30\u001b[0m cols \u001b[38;5;241m=\u001b[39m tdf\u001b[38;5;241m.\u001b[39mcolumns[tdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO2 -\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: MinMaxScaler.transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for station in stations:\n",
    "    df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "    # Limiting Date\n",
    "    df = df[df[\"DATE\"]<\"2020-01-01\"]\n",
    "\n",
    "    tdf = df[[\"DATE\",\"SPA.NO2\"]].copy()\n",
    "    tdf.columns = [\"DATE\",\"NO2\"]\n",
    "    tdf[\"NO2\"]=np.log1p(tdf[\"NO2\"])\n",
    "\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    stl = seasonal_decompose(tdf[\"NO2\"], model=\"additive\",period=24)\n",
    "    tdf[\"NO2\"]=tdf[\"NO2\"]-stl.seasonal\n",
    "    tdf[\"trend\"]=stl.trend\n",
    "\n",
    "    horizon=13\n",
    "\n",
    "    tdf[\"trend_norm\"]=tdf[\"trend\"].shift(horizon)\n",
    "\n",
    "    # We remove the trend and keep past values\n",
    "    tdf[\"NO2\"]=tdf[\"NO2\"]-tdf[\"trend_norm\"]\n",
    "    for h in np.arange(0,horizon+seq_length+1):\n",
    "        if h>horizon:\n",
    "            tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
    "            tdf[f\"NO2 - {h}\"]=scaler.transform()\n",
    "\n",
    "    tdf=tdf.dropna()\n",
    "\n",
    "    cols = tdf.columns[tdf.columns.str.contains(\"NO2 -\")]\n",
    "    X = tdf[cols].copy()\n",
    "    y = tdf[[\"NO2\"]].copy()\n",
    "    \n",
    "    TRAIN_SPLIT = tdf[tdf[\"DATE\"]>\"2018\"].index.values[0]\n",
    "\n",
    "    X_train.append(X[X.index<=TRAIN_SPLIT].copy())\n",
    "    y_train.append(y[X.index<=TRAIN_SPLIT].copy())\n",
    "X_train=pd.concat(X_train)\n",
    "y_train=pd.concat(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba9ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56772, 168)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5616ffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO2 - 14</th>\n",
       "      <th>NO2 - 15</th>\n",
       "      <th>NO2 - 16</th>\n",
       "      <th>NO2 - 17</th>\n",
       "      <th>NO2 - 18</th>\n",
       "      <th>NO2 - 19</th>\n",
       "      <th>NO2 - 20</th>\n",
       "      <th>NO2 - 21</th>\n",
       "      <th>NO2 - 22</th>\n",
       "      <th>NO2 - 23</th>\n",
       "      <th>...</th>\n",
       "      <th>NO2 - 172</th>\n",
       "      <th>NO2 - 173</th>\n",
       "      <th>NO2 - 174</th>\n",
       "      <th>NO2 - 175</th>\n",
       "      <th>NO2 - 176</th>\n",
       "      <th>NO2 - 177</th>\n",
       "      <th>NO2 - 178</th>\n",
       "      <th>NO2 - 179</th>\n",
       "      <th>NO2 - 180</th>\n",
       "      <th>NO2 - 181</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-2.996216</td>\n",
       "      <td>-2.799638</td>\n",
       "      <td>-2.690678</td>\n",
       "      <td>-2.666934</td>\n",
       "      <td>-3.028388</td>\n",
       "      <td>-3.489520</td>\n",
       "      <td>-3.971476</td>\n",
       "      <td>-3.925207</td>\n",
       "      <td>-3.004463</td>\n",
       "      <td>-2.872405</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.400338</td>\n",
       "      <td>-3.655107</td>\n",
       "      <td>-3.536014</td>\n",
       "      <td>-3.722969</td>\n",
       "      <td>-3.591644</td>\n",
       "      <td>-3.319321</td>\n",
       "      <td>-3.109680</td>\n",
       "      <td>-3.173329</td>\n",
       "      <td>-3.347363</td>\n",
       "      <td>-3.248272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-3.175049</td>\n",
       "      <td>-2.946764</td>\n",
       "      <td>-2.750186</td>\n",
       "      <td>-2.641226</td>\n",
       "      <td>-2.617483</td>\n",
       "      <td>-2.978936</td>\n",
       "      <td>-3.440068</td>\n",
       "      <td>-3.922024</td>\n",
       "      <td>-3.875756</td>\n",
       "      <td>-2.955011</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.089770</td>\n",
       "      <td>-3.350886</td>\n",
       "      <td>-3.605656</td>\n",
       "      <td>-3.486562</td>\n",
       "      <td>-3.673517</td>\n",
       "      <td>-3.542193</td>\n",
       "      <td>-3.269869</td>\n",
       "      <td>-3.060228</td>\n",
       "      <td>-3.123877</td>\n",
       "      <td>-3.297912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-3.346077</td>\n",
       "      <td>-3.140174</td>\n",
       "      <td>-2.911890</td>\n",
       "      <td>-2.715312</td>\n",
       "      <td>-2.606352</td>\n",
       "      <td>-2.582608</td>\n",
       "      <td>-2.944062</td>\n",
       "      <td>-3.405194</td>\n",
       "      <td>-3.887149</td>\n",
       "      <td>-3.840881</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.858500</td>\n",
       "      <td>-3.054895</td>\n",
       "      <td>-3.316011</td>\n",
       "      <td>-3.570781</td>\n",
       "      <td>-3.451688</td>\n",
       "      <td>-3.638642</td>\n",
       "      <td>-3.507318</td>\n",
       "      <td>-3.234995</td>\n",
       "      <td>-3.025354</td>\n",
       "      <td>-3.089003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-3.150227</td>\n",
       "      <td>-3.319345</td>\n",
       "      <td>-3.113443</td>\n",
       "      <td>-2.885158</td>\n",
       "      <td>-2.688581</td>\n",
       "      <td>-2.579620</td>\n",
       "      <td>-2.555877</td>\n",
       "      <td>-2.917330</td>\n",
       "      <td>-3.378462</td>\n",
       "      <td>-3.860418</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.873764</td>\n",
       "      <td>-2.831769</td>\n",
       "      <td>-3.028164</td>\n",
       "      <td>-3.289280</td>\n",
       "      <td>-3.544050</td>\n",
       "      <td>-3.424956</td>\n",
       "      <td>-3.611911</td>\n",
       "      <td>-3.480587</td>\n",
       "      <td>-3.208263</td>\n",
       "      <td>-2.998622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-3.084282</td>\n",
       "      <td>-3.159633</td>\n",
       "      <td>-3.328752</td>\n",
       "      <td>-3.122849</td>\n",
       "      <td>-2.894565</td>\n",
       "      <td>-2.697987</td>\n",
       "      <td>-2.589026</td>\n",
       "      <td>-2.565283</td>\n",
       "      <td>-2.926736</td>\n",
       "      <td>-3.387868</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.772771</td>\n",
       "      <td>-2.883171</td>\n",
       "      <td>-2.841175</td>\n",
       "      <td>-3.037570</td>\n",
       "      <td>-3.298686</td>\n",
       "      <td>-3.553456</td>\n",
       "      <td>-3.434362</td>\n",
       "      <td>-3.621317</td>\n",
       "      <td>-3.489993</td>\n",
       "      <td>-3.217669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NO2 - 14  NO2 - 15  NO2 - 16  NO2 - 17  NO2 - 18  NO2 - 19  NO2 - 20  \\\n",
       "206 -2.996216 -2.799638 -2.690678 -2.666934 -3.028388 -3.489520 -3.971476   \n",
       "207 -3.175049 -2.946764 -2.750186 -2.641226 -2.617483 -2.978936 -3.440068   \n",
       "208 -3.346077 -3.140174 -2.911890 -2.715312 -2.606352 -2.582608 -2.944062   \n",
       "209 -3.150227 -3.319345 -3.113443 -2.885158 -2.688581 -2.579620 -2.555877   \n",
       "210 -3.084282 -3.159633 -3.328752 -3.122849 -2.894565 -2.697987 -2.589026   \n",
       "\n",
       "     NO2 - 21  NO2 - 22  NO2 - 23  ...  NO2 - 172  NO2 - 173  NO2 - 174  \\\n",
       "206 -3.925207 -3.004463 -2.872405  ...  -3.400338  -3.655107  -3.536014   \n",
       "207 -3.922024 -3.875756 -2.955011  ...  -3.089770  -3.350886  -3.605656   \n",
       "208 -3.405194 -3.887149 -3.840881  ...  -2.858500  -3.054895  -3.316011   \n",
       "209 -2.917330 -3.378462 -3.860418  ...  -2.873764  -2.831769  -3.028164   \n",
       "210 -2.565283 -2.926736 -3.387868  ...  -2.772771  -2.883171  -2.841175   \n",
       "\n",
       "     NO2 - 175  NO2 - 176  NO2 - 177  NO2 - 178  NO2 - 179  NO2 - 180  \\\n",
       "206  -3.722969  -3.591644  -3.319321  -3.109680  -3.173329  -3.347363   \n",
       "207  -3.486562  -3.673517  -3.542193  -3.269869  -3.060228  -3.123877   \n",
       "208  -3.570781  -3.451688  -3.638642  -3.507318  -3.234995  -3.025354   \n",
       "209  -3.289280  -3.544050  -3.424956  -3.611911  -3.480587  -3.208263   \n",
       "210  -3.037570  -3.298686  -3.553456  -3.434362  -3.621317  -3.489993   \n",
       "\n",
       "     NO2 - 181  \n",
       "206  -3.248272  \n",
       "207  -3.297912  \n",
       "208  -3.089003  \n",
       "209  -2.998622  \n",
       "210  -3.217669  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5902143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9462614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2347121095.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n"
     ]
    }
   ],
   "source": [
    "station=\"28079004\"\n",
    "df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "# Limiting Date\n",
    "df = df[df[\"DATE\"]<\"2020-01-01\"]\n",
    "\n",
    "tdf = df[[\"DATE\",\"SPA.NO2\"]].copy()\n",
    "tdf.columns = [\"DATE\",\"NO2\"]\n",
    "tdf[\"NO2\"]=np.log1p(tdf[\"NO2\"])\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "stl = seasonal_decompose(tdf[\"NO2\"], model=\"additive\",period=24)\n",
    "tdf[\"NO2\"]=tdf[\"NO2\"]-stl.seasonal\n",
    "tdf[\"trend\"]=stl.trend\n",
    "\n",
    "horizon=13\n",
    "\n",
    "tdf[\"trend_norm\"]=tdf[\"trend\"].shift(horizon)\n",
    "\n",
    "# We remove the trend and keep past values\n",
    "tdf[\"NO2\"]=tdf[\"NO2\"]-tdf[\"trend_norm\"]\n",
    "for h in np.arange(0,horizon+seq_length+1):\n",
    "    if h>horizon:\n",
    "        tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
    "\n",
    "tdf=tdf.dropna()\n",
    "\n",
    "cols = tdf.columns[tdf.columns.str.contains(\"NO2 -\")]\n",
    "X = tdf[cols].copy()\n",
    "y = tdf[[\"NO2\"]].copy()\n",
    "\n",
    "TRAIN_SPLIT = tdf[tdf[\"DATE\"]>\"2018\"].index.values[0]\n",
    "\n",
    "X_test=X[X.index>TRAIN_SPLIT].copy()\n",
    "y_test = y[X.index>TRAIN_SPLIT].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fe0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = torch.FloatTensor(X_train.values).to(device)\n",
    "X_test = torch.FloatTensor(X_test.values).to(device)\n",
    "y_train = torch.FloatTensor(y_train.values).to(device)\n",
    "y_test= torch.FloatTensor(y_test.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9bee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harryliew/COCOB-optimizer/blob/master/cocob_bp.py\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "###########################################################################\n",
    "# Training Deep Networks without Learning Rates Through Coin Betting\n",
    "# Paper: https://arxiv.org/abs/1705.07795\n",
    "#\n",
    "# NOTE: This optimizer is hardcoded to run on GPU, needs to be parametrized\n",
    "###########################################################################\n",
    "\n",
    "class COCOBBackprop(optim.Optimizer):\n",
    "    \n",
    "    def __init__(self, params, alpha=100, epsilon=1e-8):\n",
    "        \n",
    "        self._alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        defaults = dict(alpha=alpha, epsilon=epsilon)\n",
    "        super(COCOBBackprop, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "        \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state) == 0:\n",
    "                    state['gradients_sum'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['grad_norm_sum'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['L'] = self.epsilon * torch.ones_like(p.data).cuda().float()\n",
    "                    state['tilde_w'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['reward'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    \n",
    "                gradients_sum = state['gradients_sum']\n",
    "                grad_norm_sum = state['grad_norm_sum']\n",
    "                tilde_w = state['tilde_w']\n",
    "                L = state['L']\n",
    "                reward = state['reward']\n",
    "                \n",
    "                zero = torch.cuda.FloatTensor([0.])\n",
    "                \n",
    "                L_update = torch.max(L, torch.abs(grad))\n",
    "                gradients_sum_update = gradients_sum + grad\n",
    "                grad_norm_sum_update = grad_norm_sum + torch.abs(grad)\n",
    "                reward_update = torch.max(reward - grad * tilde_w, zero)\n",
    "                new_w = -gradients_sum_update/(L_update * (torch.max(grad_norm_sum_update + L_update, self._alpha * L_update)))*(reward_update + L_update)\n",
    "                p.data = p.data - tilde_w + new_w\n",
    "                tilde_w_update = new_w\n",
    "                \n",
    "                state['gradients_sum'] = gradients_sum_update\n",
    "                state['grad_norm_sum'] = grad_norm_sum_update\n",
    "                state['L'] = L_update\n",
    "                state['tilde_w'] = tilde_w_update\n",
    "                state['reward'] = reward_update\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4fffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3daf91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas/miniconda3/envs/phd/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4585537530891118"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbmodel = lgb.LGBMRegressor(n_estimators=1600,\n",
    "                                    random_state=2020,\n",
    "                                  max_depth=6)\n",
    "\n",
    "\n",
    "lgbmodel.fit(X_train.cpu(),y_train.cpu())\n",
    "predictions = lgbmodel.predict(X_test.cpu())\n",
    "np.sqrt(mean_squared_error(predictions,y_test.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00d36809",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 882.078369140625 Val Loss: 0.5082942461847594\n",
      "Epoch: 1 Loss: 817.6134033203125 Val Loss: 0.5370150952155733\n",
      "Epoch: 2 Loss: 769.1175537109375 Val Loss: 0.5441856216771758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     51\u001b[0m     m_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     53\u001b[0m         train_features, train_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     54\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(train_features\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;28mlen\u001b[39m(train_features), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/utils/data/dataloader.py:569\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/utils/data/sampler.py:227\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m--> 227\u001b[0m     \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.input_size = 1\n",
    "        self.output_size=1\n",
    "        self.hidden_dim=64\n",
    "        self.n_layers=2\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            self.input_size, self.hidden_dim, self.n_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = GRUModel() \n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.01,weight_decay=0.001)\n",
    "batch_size = 3000\n",
    "epochs = 6\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        train_features, train_labels = batch\n",
    "        y_hat = net.forward(train_features.reshape([len(train_features), -1, 1])).to(device)\n",
    "        loss = criterion(y_hat, train_labels)\n",
    "        m_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "    predictions=np.array([])\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        test_b = batch[0]\n",
    "        predictions = np.concatenate([predictions,net(test_b.reshape([len(test_b), seq_length,1])).cpu().reshape(-1).detach().numpy()])\n",
    "    #loss_arr.append(np.mean(m_loss))\n",
    "    #val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e0b28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 586.9235229492188 Val Loss: 0.4978141213669613\n",
      "Epoch: 1 Loss: 566.3613891601562 Val Loss: 0.5075631439468482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(train_features\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;28mlen\u001b[39m(train_features), seq_length ,\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, train_labels)\n\u001b[0;32m---> 45\u001b[0m m_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.input_size = 1\n",
    "    self.output_size=1\n",
    "    self.hidden_dim=64\n",
    "    self.n_layers=2\n",
    "\n",
    "    super(Net,self).__init__() \n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_dim, self.n_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(self.hidden_dim*seq_length, self.output_size)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    self.lstm.flatten_parameters()\n",
    "    h_0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "    c_0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "    \n",
    "    out, (hidden,_) = self.lstm(x,(h_0.detach(), c_0.detach()))\n",
    "    out=out.reshape([len(x),self.hidden_dim*seq_length])\n",
    "    # get final output \n",
    "    output = self.fc(out)\n",
    "    \n",
    "    return output\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "#optimizer = COCOBBackprop(net.parameters())\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.00001,weight_decay=0.000001)\n",
    "batch_size = 2000\n",
    "epochs = 6\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        train_features, train_labels = batch\n",
    "        y_hat = net.forward(train_features.reshape([len(train_features), seq_length ,1])).to(device)\n",
    "        loss = criterion(y_hat, train_labels)\n",
    "        m_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "    predictions=np.array([])\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        test_b = batch[0]\n",
    "        predictions = np.concatenate([predictions,net(test_b.reshape([len(test_b), seq_length,1])).cpu().reshape(-1).detach().numpy()])\n",
    "    #loss_arr.append(np.mean(m_loss))\n",
    "    #val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ffaa389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1011.3659057617188 Val Loss: 0.47193238139152527\n",
      "Epoch: 1 Loss: 543.7345581054688 Val Loss: 0.46646928787231445\n",
      "Epoch: 2 Loss: 541.357177734375 Val Loss: 0.4671001434326172\n",
      "Epoch: 3 Loss: 540.1487426757812 Val Loss: 0.466011106967926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(train_features)\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, train_labels)\n\u001b[0;32m---> 55\u001b[0m m_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "n_channels = 64\n",
    "k_size = 3\n",
    "n_dil = 2\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net,self).__init__() \n",
    "    self.features = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=n_channels, kernel_size=k_size, padding=1),\n",
    "        nn.MaxPool1d(2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    self.regressor = nn.Sequential(\n",
    "        nn.LayerNorm(n_channels*84),\n",
    "        nn.Linear(n_channels*84,200),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(200),\n",
    "        nn.Linear(200,1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.features(x)\n",
    "    #print(x.shape)\n",
    "    x = x.view(-1,n_channels*x.shape[2])\n",
    "    return self.regressor(x)\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "lr=0.01\n",
    "wd=lr/100\n",
    "optimizer = optim.Adam(net.parameters(),lr=lr,weight_decay=wd)\n",
    "\n",
    "loss_arr = []\n",
    "val_loss_arr = []\n",
    "epochs = 10\n",
    "\n",
    "X_train_gf = X_train.reshape(len(X_train),1,len(X_train[0]))\n",
    "train = TensorDataset(X_train_gf, y_train)\n",
    "train_loader = DataLoader(train, batch_size=2048, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "      train_features, train_labels = batch\n",
    "      y_hat = net.forward(train_features)\n",
    "      loss = criterion(y_hat, train_labels)\n",
    "      m_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    loss_arr.append(np.mean(m_loss))\n",
    "\n",
    "    X_test_gf = X_test.reshape(len(X_test),1,len(X_test[0]))\n",
    "    predictions = net(X_test_gf).cpu().reshape(-1).detach().numpy()\n",
    "    val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099ba348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 68.74227905273438 Val Loss: 0.46806034445762634\n",
      "Epoch: 1 Loss: 66.00784301757812 Val Loss: 0.4643477201461792\n",
      "Epoch: 2 Loss: 64.43340301513672 Val Loss: 0.4651913344860077\n",
      "Epoch: 3 Loss: 63.219181060791016 Val Loss: 0.4651828706264496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 73\u001b[0m   \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m X_test_gf \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(X_test),\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X_test[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     77\u001b[0m predictions \u001b[38;5;241m=\u001b[39m net(X_test_gf)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mCOCOBBackprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     47\u001b[0m L \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     48\u001b[0m reward \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m zero \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m L_update \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(L, torch\u001b[38;5;241m.\u001b[39mabs(grad))\n\u001b[1;32m     53\u001b[0m gradients_sum_update \u001b[38;5;241m=\u001b[39m gradients_sum \u001b[38;5;241m+\u001b[39m grad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_channels = 32\n",
    "k_size = 3\n",
    "n_dil = 2 \n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "           nn.Conv1d(in_channels=1, out_channels=n_channels, kernel_size=k_size, padding=1),\n",
    "           nn.MaxPool1d(2),\n",
    "        )\n",
    "        \n",
    "        # GRU layers\n",
    "        self.output_size=1\n",
    "        self.hidden_dim=64\n",
    "        self.n_layers=1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            n_channels, self.hidden_dim, self.n_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        feat_in = self.features(x)\n",
    "        #print(f\"feat_in: {feat_in.shape}\")\n",
    "        feat_in = feat_in.reshape([len(feat_in), 84 ,32])\n",
    "        \n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(feat_in, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = GRUModel() \n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "#optimizer = optim.Adam(net.parameters(),lr=0.01,weight_decay=0.001)\n",
    "batch_size = 256\n",
    "epochs = 6\n",
    "\n",
    "X_train_gf = X_train.reshape(len(X_train),1,len(X_train[0]))\n",
    "train = TensorDataset(X_train_gf, y_train)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "      train_features, train_labels = batch\n",
    "      y_hat = net.forward(train_features)\n",
    "      loss = criterion(y_hat, train_labels)\n",
    "      m_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "\n",
    "    X_test_gf = X_test.reshape(len(X_test),1,len(X_test[0]))\n",
    "    predictions = net(X_test_gf).cpu().reshape(-1).detach().numpy()\n",
    "    #val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4911dd",
   "metadata": {},
   "source": [
    "# Test solo station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d16f32aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
      "/tmp/ipykernel_2375/2698257401.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n"
     ]
    }
   ],
   "source": [
    "X_trains=[]\n",
    "y_trains=[]\n",
    "station=\"28079004\"\n",
    "\n",
    "df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "# Limiting Date\n",
    "df = df[df[\"DATE\"]<\"2020-01-01\"]\n",
    "\n",
    "tdf = df[[\"DATE\",\"SPA.NO2\"]].copy()\n",
    "tdf.columns = [\"DATE\",\"NO2\"]\n",
    "tdf[\"NO2\"]=np.log1p(tdf[\"NO2\"])\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "stl = seasonal_decompose(tdf[\"NO2\"], model=\"additive\",period=24)\n",
    "tdf[\"NO2\"]=tdf[\"NO2\"]-stl.seasonal\n",
    "tdf[\"trend\"]=stl.trend\n",
    "\n",
    "horizon=13\n",
    "\n",
    "tdf[\"trend_norm\"]=tdf[\"trend\"].shift(horizon)\n",
    "\n",
    "# We remove the trend and keep past values\n",
    "tdf[\"NO2\"]=tdf[\"NO2\"]-tdf[\"trend_norm\"]\n",
    "for h in np.arange(0,horizon+seq_length+1):\n",
    "    if h>horizon:\n",
    "        tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
    "\n",
    "tdf=tdf.dropna()\n",
    "\n",
    "cols = tdf.columns[tdf.columns.str.contains(\"NO2 -\")]\n",
    "X = tdf[cols].copy()\n",
    "y = tdf[[\"NO2\"]].copy()\n",
    "\n",
    "TRAIN_SPLIT = tdf[tdf[\"DATE\"]>\"2018\"].index.values[0]\n",
    "\n",
    "X_trains=X[X.index<=TRAIN_SPLIT].copy()\n",
    "y_trains=y[X.index<=TRAIN_SPLIT].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cbe4a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4744700562762055"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbmodel = lgb.LGBMRegressor(n_estimators=1600,\n",
    "                                    random_state=2020,\n",
    "                                  max_depth=6)\n",
    "\n",
    "\n",
    "lgbmodel.fit(X_trains,y_trains)\n",
    "predictions = lgbmodel.predict(X_test.cpu())\n",
    "np.sqrt(mean_squared_error(predictions,y_test.cpu()))\n",
    "#0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b13854de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 47600.953125 Val Loss: 0.5063426494598389\n",
      "Epoch: 1 Loss: 1808.752685546875 Val Loss: 0.49405044317245483\n",
      "Epoch: 2 Loss: 1755.691162109375 Val Loss: 0.4936702251434326\n",
      "Epoch: 3 Loss: 1753.1826171875 Val Loss: 0.4932498633861542\n",
      "Epoch: 4 Loss: 1750.383056640625 Val Loss: 0.49275586009025574\n",
      "Epoch: 5 Loss: 1747.297119140625 Val Loss: 0.4921426475048065\n",
      "Epoch: 6 Loss: 1743.7203369140625 Val Loss: 0.4914079010486603\n",
      "Epoch: 7 Loss: 1738.0550537109375 Val Loss: 0.4903293550014496\n",
      "Epoch: 8 Loss: 1729.956787109375 Val Loss: 0.4887824058532715\n",
      "Epoch: 9 Loss: 1718.042724609375 Val Loss: 0.4861428737640381\n",
      "Epoch: 10 Loss: 1699.183349609375 Val Loss: 0.4819730818271637\n",
      "Epoch: 11 Loss: 1671.3616943359375 Val Loss: 0.47864410281181335\n",
      "Epoch: 12 Loss: 1704.7587890625 Val Loss: 0.47812405228614807\n",
      "Epoch: 13 Loss: 1646.2503662109375 Val Loss: 0.47287416458129883\n",
      "Epoch: 14 Loss: 1623.87109375 Val Loss: 0.47067248821258545\n",
      "Epoch: 15 Loss: 1622.4967041015625 Val Loss: 0.4708791673183441\n",
      "Epoch: 16 Loss: 1619.652099609375 Val Loss: 0.46979257464408875\n",
      "Epoch: 17 Loss: 1629.6898193359375 Val Loss: 0.4709242582321167\n",
      "Epoch: 18 Loss: 1639.4085693359375 Val Loss: 0.4691941440105438\n",
      "Epoch: 19 Loss: 1601.392822265625 Val Loss: 0.4668082892894745\n",
      "Epoch: 20 Loss: 1635.402099609375 Val Loss: 0.4700881540775299\n",
      "Epoch: 21 Loss: 1600.8922119140625 Val Loss: 0.4668933153152466\n",
      "Epoch: 22 Loss: 1592.7662353515625 Val Loss: 0.4662695527076721\n",
      "Epoch: 23 Loss: 1627.200927734375 Val Loss: 0.46821707487106323\n",
      "Epoch: 24 Loss: 1595.171630859375 Val Loss: 0.46668893098831177\n",
      "Epoch: 25 Loss: 1595.48193359375 Val Loss: 0.4687640368938446\n",
      "Epoch: 26 Loss: 1607.0107421875 Val Loss: 0.4665527939796448\n",
      "Epoch: 27 Loss: 1586.3626708984375 Val Loss: 0.4669458270072937\n",
      "Epoch: 28 Loss: 1589.1767578125 Val Loss: 0.466168075799942\n",
      "Epoch: 29 Loss: 1590.55078125 Val Loss: 0.4690796136856079\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "n_channels = 64\n",
    "k_size = 3\n",
    "n_dil = 2\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net,self).__init__() \n",
    "    self.features = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=n_channels, kernel_size=k_size, padding=1),\n",
    "        nn.MaxPool1d(2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    self.regressor = nn.Sequential(\n",
    "        nn.LayerNorm(n_channels*84),\n",
    "        nn.Linear(n_channels*84,200),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(200),\n",
    "        nn.Linear(200,1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.features(x)\n",
    "    #print(x.shape)\n",
    "    x = x.view(-1,n_channels*x.shape[2])\n",
    "    return self.regressor(x)\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "\n",
    "loss_arr = []\n",
    "val_loss_arr = []\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "X_trains = torch.FloatTensor(X_trains.values).to(device)\n",
    "y_trains = torch.FloatTensor(y_trains.values).to(device)\n",
    "X_trains_gf = X_trains.reshape(len(X_trains),1,len(X_trains[0]))\n",
    "train = TensorDataset(X_trains_gf, y_trains)\n",
    "train_loader = DataLoader(train, batch_size=4096*2, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "      train_features, train_labels = batch\n",
    "      y_hat = net.forward(train_features)\n",
    "      loss = criterion(y_hat, train_labels)\n",
    "      m_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    loss_arr.append(np.mean(m_loss))\n",
    "\n",
    "    X_test_gf = X_test.reshape(len(X_test),1,len(X_test[0]))\n",
    "    predictions = net(X_test_gf).cpu().reshape(-1).detach().numpy()\n",
    "    val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74d94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
