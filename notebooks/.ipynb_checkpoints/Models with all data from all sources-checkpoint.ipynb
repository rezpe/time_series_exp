{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98f22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b15708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stations = os.listdir(\"/home/sebas/data/air_qual_aemet/\")\n",
    "stations = pd.Series(stations).apply(lambda e: e.split(\".csv\")[0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364bff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56ab293",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for station in stations:\n",
    "    df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "    # Limiting Date\n",
    "    df = df[df[\"DATE\"]<\"2020-01-01\"]\n",
    "\n",
    "    tdf = df[[\"DATE\",\"SPA.NO2\"]].copy()\n",
    "    tdf.columns = [\"DATE\",\"NO2\"]\n",
    "    tdf[\"NO2\"]=np.log1p(tdf[\"NO2\"])\n",
    "\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    stl = seasonal_decompose(tdf[\"NO2\"], model=\"additive\",period=24)\n",
    "    tdf[\"NO2\"]=tdf[\"NO2\"]-stl.seasonal\n",
    "    tdf[\"trend\"]=stl.trend\n",
    "\n",
    "    horizon=13\n",
    "\n",
    "    tdf[\"trend_norm\"]=tdf[\"trend\"].shift(horizon)\n",
    "\n",
    "    # We remove the trend and keep past values\n",
    "    for h in np.arange(0,horizon+seq_length+1):\n",
    "        if h>horizon:\n",
    "            tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
    "\n",
    "    tdf=tdf.dropna()\n",
    "\n",
    "    cols = tdf.columns[tdf.columns.str.contains(\"NO2 -\")]\n",
    "    X = tdf[cols].copy()\n",
    "    y = tdf[[\"NO2\"]].copy()\n",
    "    \n",
    "    TRAIN_SPLIT = tdf[tdf[\"DATE\"]>\"2018\"].index.values[0]\n",
    "\n",
    "    X_train.append(X[X.index<=TRAIN_SPLIT].copy())\n",
    "    y_train.append(y[X.index<=TRAIN_SPLIT].copy())\n",
    "X_train=pd.concat(X_train)\n",
    "y_train=pd.concat(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba9ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56945, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5616ffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO2 - 14</th>\n",
       "      <th>NO2 - 15</th>\n",
       "      <th>NO2 - 16</th>\n",
       "      <th>NO2 - 17</th>\n",
       "      <th>NO2 - 18</th>\n",
       "      <th>NO2 - 19</th>\n",
       "      <th>NO2 - 20</th>\n",
       "      <th>NO2 - 21</th>\n",
       "      <th>NO2 - 22</th>\n",
       "      <th>NO2 - 23</th>\n",
       "      <th>NO2 - 24</th>\n",
       "      <th>NO2 - 25</th>\n",
       "      <th>NO2 - 26</th>\n",
       "      <th>NO2 - 27</th>\n",
       "      <th>NO2 - 28</th>\n",
       "      <th>NO2 - 29</th>\n",
       "      <th>NO2 - 30</th>\n",
       "      <th>NO2 - 31</th>\n",
       "      <th>NO2 - 32</th>\n",
       "      <th>NO2 - 33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.689521</td>\n",
       "      <td>0.286442</td>\n",
       "      <td>0.492656</td>\n",
       "      <td>0.130664</td>\n",
       "      <td>-0.117773</td>\n",
       "      <td>-0.366197</td>\n",
       "      <td>-0.397732</td>\n",
       "      <td>-0.134157</td>\n",
       "      <td>-0.200466</td>\n",
       "      <td>-0.091516</td>\n",
       "      <td>0.156779</td>\n",
       "      <td>0.503612</td>\n",
       "      <td>-0.163248</td>\n",
       "      <td>-0.434935</td>\n",
       "      <td>-0.040562</td>\n",
       "      <td>0.143331</td>\n",
       "      <td>0.151701</td>\n",
       "      <td>-0.093511</td>\n",
       "      <td>-0.009911</td>\n",
       "      <td>0.162708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.398445</td>\n",
       "      <td>0.722583</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.525717</td>\n",
       "      <td>0.163726</td>\n",
       "      <td>-0.084711</td>\n",
       "      <td>-0.333135</td>\n",
       "      <td>-0.364670</td>\n",
       "      <td>-0.101095</td>\n",
       "      <td>-0.167405</td>\n",
       "      <td>-0.058454</td>\n",
       "      <td>0.189841</td>\n",
       "      <td>0.536674</td>\n",
       "      <td>-0.130186</td>\n",
       "      <td>-0.401874</td>\n",
       "      <td>-0.007501</td>\n",
       "      <td>0.176393</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>-0.060449</td>\n",
       "      <td>0.023150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.264320</td>\n",
       "      <td>0.417534</td>\n",
       "      <td>0.741672</td>\n",
       "      <td>0.338594</td>\n",
       "      <td>0.544807</td>\n",
       "      <td>0.182815</td>\n",
       "      <td>-0.065622</td>\n",
       "      <td>-0.314046</td>\n",
       "      <td>-0.345581</td>\n",
       "      <td>-0.082006</td>\n",
       "      <td>-0.148315</td>\n",
       "      <td>-0.039365</td>\n",
       "      <td>0.208931</td>\n",
       "      <td>0.555763</td>\n",
       "      <td>-0.111097</td>\n",
       "      <td>-0.382784</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.195482</td>\n",
       "      <td>0.203852</td>\n",
       "      <td>-0.041360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.134545</td>\n",
       "      <td>0.266306</td>\n",
       "      <td>0.419520</td>\n",
       "      <td>0.743658</td>\n",
       "      <td>0.340579</td>\n",
       "      <td>0.546792</td>\n",
       "      <td>0.184801</td>\n",
       "      <td>-0.063636</td>\n",
       "      <td>-0.312060</td>\n",
       "      <td>-0.343595</td>\n",
       "      <td>-0.080020</td>\n",
       "      <td>-0.146329</td>\n",
       "      <td>-0.037379</td>\n",
       "      <td>0.210916</td>\n",
       "      <td>0.557749</td>\n",
       "      <td>-0.109111</td>\n",
       "      <td>-0.380798</td>\n",
       "      <td>0.013574</td>\n",
       "      <td>0.197468</td>\n",
       "      <td>0.205837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.200597</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.258311</td>\n",
       "      <td>0.411525</td>\n",
       "      <td>0.735662</td>\n",
       "      <td>0.332584</td>\n",
       "      <td>0.538797</td>\n",
       "      <td>0.176806</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.320055</td>\n",
       "      <td>-0.351590</td>\n",
       "      <td>-0.088015</td>\n",
       "      <td>-0.154325</td>\n",
       "      <td>-0.045375</td>\n",
       "      <td>0.202921</td>\n",
       "      <td>0.549754</td>\n",
       "      <td>-0.117107</td>\n",
       "      <td>-0.388794</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.189473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NO2 - 14  NO2 - 15  NO2 - 16  NO2 - 17  NO2 - 18  NO2 - 19  NO2 - 20  \\\n",
       "33  0.689521  0.286442  0.492656  0.130664 -0.117773 -0.366197 -0.397732   \n",
       "34  0.398445  0.722583  0.319504  0.525717  0.163726 -0.084711 -0.333135   \n",
       "35  0.264320  0.417534  0.741672  0.338594  0.544807  0.182815 -0.065622   \n",
       "36  0.134545  0.266306  0.419520  0.743658  0.340579  0.546792  0.184801   \n",
       "37  0.200597  0.126549  0.258311  0.411525  0.735662  0.332584  0.538797   \n",
       "\n",
       "    NO2 - 21  NO2 - 22  NO2 - 23  NO2 - 24  NO2 - 25  NO2 - 26  NO2 - 27  \\\n",
       "33 -0.134157 -0.200466 -0.091516  0.156779  0.503612 -0.163248 -0.434935   \n",
       "34 -0.364670 -0.101095 -0.167405 -0.058454  0.189841  0.536674 -0.130186   \n",
       "35 -0.314046 -0.345581 -0.082006 -0.148315 -0.039365  0.208931  0.555763   \n",
       "36 -0.063636 -0.312060 -0.343595 -0.080020 -0.146329 -0.037379  0.210916   \n",
       "37  0.176806 -0.071632 -0.320055 -0.351590 -0.088015 -0.154325 -0.045375   \n",
       "\n",
       "    NO2 - 28  NO2 - 29  NO2 - 30  NO2 - 31  NO2 - 32  NO2 - 33  \n",
       "33 -0.040562  0.143331  0.151701 -0.093511 -0.009911  0.162708  \n",
       "34 -0.401874 -0.007501  0.176393  0.184762 -0.060449  0.023150  \n",
       "35 -0.111097 -0.382784  0.011589  0.195482  0.203852 -0.041360  \n",
       "36  0.557749 -0.109111 -0.380798  0.013574  0.197468  0.205837  \n",
       "37  0.202921  0.549754 -0.117107 -0.388794  0.005579  0.189473  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5902143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946512"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9462614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station=\"28079004\"\n",
    "df = pd.read_csv(f\"/home/sebas/data/air_qual_aemet/{station}.csv\",sep=\";\")\n",
    "# Limiting Date\n",
    "df = df[df[\"DATE\"]<\"2020-01-01\"]\n",
    "\n",
    "tdf = df[[\"DATE\",\"SPA.NO2\"]].copy()\n",
    "tdf.columns = [\"DATE\",\"NO2\"]\n",
    "tdf[\"NO2\"]=np.log1p(tdf[\"NO2\"])\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "stl = seasonal_decompose(tdf[\"NO2\"], model=\"additive\",period=24)\n",
    "tdf[\"NO2\"]=tdf[\"NO2\"]-stl.seasonal\n",
    "tdf[\"trend\"]=stl.trend\n",
    "\n",
    "horizon=13\n",
    "\n",
    "tdf[\"trend_norm\"]=tdf[\"trend\"].shift(horizon)\n",
    "\n",
    "# We remove the trend and keep past values\n",
    "for h in np.arange(0,horizon+seq_length+1):\n",
    "    if h>horizon:\n",
    "        tdf[f\"NO2 - {h}\"]=(tdf[\"NO2\"].shift(h)-tdf[\"trend_norm\"]).copy()\n",
    "\n",
    "tdf=tdf.dropna()\n",
    "\n",
    "cols = tdf.columns[tdf.columns.str.contains(\"NO2 -\")]\n",
    "X = tdf[cols].copy()\n",
    "y = tdf[[\"NO2\"]].copy()\n",
    "\n",
    "TRAIN_SPLIT = tdf[tdf[\"DATE\"]>\"2018\"].index.values[0]\n",
    "\n",
    "X_test=X[X.index>TRAIN_SPLIT].copy()\n",
    "y_test = y[X.index>TRAIN_SPLIT].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15fe0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = torch.FloatTensor(X_train.values).to(device)\n",
    "X_test = torch.FloatTensor(X_test.values).to(device)\n",
    "y_train = torch.FloatTensor(y_train.values).to(device)\n",
    "y_test= torch.FloatTensor(y_test.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9bee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harryliew/COCOB-optimizer/blob/master/cocob_bp.py\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "###########################################################################\n",
    "# Training Deep Networks without Learning Rates Through Coin Betting\n",
    "# Paper: https://arxiv.org/abs/1705.07795\n",
    "#\n",
    "# NOTE: This optimizer is hardcoded to run on GPU, needs to be parametrized\n",
    "###########################################################################\n",
    "\n",
    "class COCOBBackprop(optim.Optimizer):\n",
    "    \n",
    "    def __init__(self, params, alpha=100, epsilon=1e-8):\n",
    "        \n",
    "        self._alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        defaults = dict(alpha=alpha, epsilon=epsilon)\n",
    "        super(COCOBBackprop, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "        \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state) == 0:\n",
    "                    state['gradients_sum'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['grad_norm_sum'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['L'] = self.epsilon * torch.ones_like(p.data).cuda().float()\n",
    "                    state['tilde_w'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    state['reward'] = torch.zeros_like(p.data).cuda().float()\n",
    "                    \n",
    "                gradients_sum = state['gradients_sum']\n",
    "                grad_norm_sum = state['grad_norm_sum']\n",
    "                tilde_w = state['tilde_w']\n",
    "                L = state['L']\n",
    "                reward = state['reward']\n",
    "                \n",
    "                zero = torch.cuda.FloatTensor([0.])\n",
    "                \n",
    "                L_update = torch.max(L, torch.abs(grad))\n",
    "                gradients_sum_update = gradients_sum + grad\n",
    "                grad_norm_sum_update = grad_norm_sum + torch.abs(grad)\n",
    "                reward_update = torch.max(reward - grad * tilde_w, zero)\n",
    "                new_w = -gradients_sum_update/(L_update * (torch.max(grad_norm_sum_update + L_update, self._alpha * L_update)))*(reward_update + L_update)\n",
    "                p.data = p.data - tilde_w + new_w\n",
    "                tilde_w_update = new_w\n",
    "                \n",
    "                state['gradients_sum'] = gradients_sum_update\n",
    "                state['grad_norm_sum'] = grad_norm_sum_update\n",
    "                state['L'] = L_update\n",
    "                state['tilde_w'] = tilde_w_update\n",
    "                state['reward'] = reward_update\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba4fffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3daf91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas/miniconda3/envs/phd/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5491158501973189"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbmodel = lgb.LGBMRegressor(n_estimators=1600,\n",
    "                                    random_state=2020,\n",
    "                                  max_depth=6)\n",
    "\n",
    "\n",
    "lgbmodel.fit(X_train.cpu(),y_train.cpu())\n",
    "predictions = lgbmodel.predict(X_test.cpu())\n",
    "np.sqrt(mean_squared_error(predictions,y_test.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00d36809",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 527.0294799804688 Val Loss: 0.6818662179180416\n",
      "Epoch: 1 Loss: 453.62762451171875 Val Loss: 0.7017342860429596\n",
      "Epoch: 2 Loss: 446.5284423828125 Val Loss: 0.7036446157996638\n",
      "Epoch: 3 Loss: 442.8443603515625 Val Loss: 0.7042116948194425\n",
      "Epoch: 4 Loss: 440.4425048828125 Val Loss: 0.6982056374340491\n",
      "Epoch: 5 Loss: 438.6380310058594 Val Loss: 0.6971351521694107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.input_size = 1\n",
    "        self.output_size=1\n",
    "        self.hidden_dim=32\n",
    "        self.n_layers=4\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            self.input_size, self.hidden_dim, self.n_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = GRUModel() \n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "batch_size = 1024\n",
    "epochs = 6\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        train_features, train_labels = batch\n",
    "        y_hat = net.forward(train_features.reshape([len(train_features), -1, 1])).to(device)\n",
    "        loss = criterion(y_hat, train_labels)\n",
    "        m_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "    predictions=np.array([])\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        test_b = batch[0]\n",
    "        predictions = np.concatenate([predictions,net(test_b.reshape([len(test_b), seq_length,1])).cpu().reshape(-1).detach().numpy()])\n",
    "    #loss_arr.append(np.mean(m_loss))\n",
    "    #val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e0b28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 52.60538864135742 Val Loss: 0.7320603225047847\n",
      "Epoch: 1 Loss: 48.83887481689453 Val Loss: 0.7419910438306321\n",
      "Epoch: 2 Loss: 47.71658706665039 Val Loss: 0.7620641289026464\n",
      "Epoch: 3 Loss: 47.01261520385742 Val Loss: 0.721572817178938\n",
      "Epoch: 4 Loss: 46.561012268066406 Val Loss: 0.7374847435632764\n",
      "Epoch: 5 Loss: 46.18305969238281 Val Loss: 0.7341608246713468\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.input_size = 1\n",
    "    self.output_size=1\n",
    "    self.hidden_dim=64\n",
    "    self.n_layers=1\n",
    "\n",
    "    super(Net,self).__init__() \n",
    "    self.lstm = nn.LSTM(self.input_size, self.hidden_dim, self.n_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(self.hidden_dim*seq_length, self.output_size)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    self.lstm.flatten_parameters()\n",
    "    h_0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "    c_0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "    \n",
    "    out, (hidden,_) = self.lstm(x,(h_0.detach(), c_0.detach()))\n",
    "    out=out.reshape([len(x),self.hidden_dim*seq_length])\n",
    "    # get final output \n",
    "    output = self.fc(out)\n",
    "    \n",
    "    return output\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        train_features, train_labels = batch\n",
    "        y_hat = net.forward(train_features.reshape([len(train_features), seq_length ,1])).to(device)\n",
    "        loss = criterion(y_hat, train_labels)\n",
    "        m_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "    predictions=np.array([])\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        test_b = batch[0]\n",
    "        predictions = np.concatenate([predictions,net(test_b.reshape([len(test_b), seq_length,1])).cpu().reshape(-1).detach().numpy()])\n",
    "    #loss_arr.append(np.mean(m_loss))\n",
    "    #val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ffaa389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 49.85369873046875 Val Loss: 0.5193692445755005\n",
      "Epoch: 1 Loss: 46.77054977416992 Val Loss: 0.5167364478111267\n",
      "Epoch: 2 Loss: 45.88050842285156 Val Loss: 0.5166429877281189\n",
      "Epoch: 3 Loss: 45.233253479003906 Val Loss: 0.5185554623603821\n",
      "Epoch: 4 Loss: 44.686283111572266 Val Loss: 0.5209136605262756\n",
      "Epoch: 5 Loss: 44.21220779418945 Val Loss: 0.5233705043792725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     55\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m   \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss_arr\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(m_loss))\n\u001b[1;32m     59\u001b[0m X_test_gf \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(X_test),\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X_test[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mCOCOBBackprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     47\u001b[0m L \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     48\u001b[0m reward \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m zero \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m L_update \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(L, torch\u001b[38;5;241m.\u001b[39mabs(grad))\n\u001b[1;32m     53\u001b[0m gradients_sum_update \u001b[38;5;241m=\u001b[39m gradients_sum \u001b[38;5;241m+\u001b[39m grad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "n_channels = 64\n",
    "k_size = 3\n",
    "n_dil = 2\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net,self).__init__() \n",
    "    self.features = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=n_channels, kernel_size=k_size, padding=1),\n",
    "        nn.MaxPool1d(2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    self.regressor = nn.Sequential(\n",
    "        nn.LayerNorm(n_channels*29),\n",
    "        nn.Linear(n_channels*29,200),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(200),\n",
    "        nn.Linear(200,1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.features(x)\n",
    "    #print(x.shape)\n",
    "    x = x.view(-1,n_channels*x.shape[2])\n",
    "    return self.regressor(x)\n",
    "\n",
    "net = Net() \n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = COCOBBackprop(net.parameters())\n",
    "\n",
    "loss_arr = []\n",
    "val_loss_arr = []\n",
    "epochs = 10\n",
    "\n",
    "X_train_gf = X_train.reshape(len(X_train),1,len(X_train[0]))\n",
    "train = TensorDataset(X_train_gf, y_train)\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "\n",
    "for j in range(epochs):\n",
    "    m_loss = []\n",
    "    for i,batch in enumerate(train_loader):\n",
    "      train_features, train_labels = batch\n",
    "      y_hat = net.forward(train_features)\n",
    "      loss = criterion(y_hat, train_labels)\n",
    "      m_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    loss_arr.append(np.mean(m_loss))\n",
    "\n",
    "    X_test_gf = X_test.reshape(len(X_test),1,len(X_test[0]))\n",
    "    predictions = net(X_test_gf).cpu().reshape(-1).detach().numpy()\n",
    "    val_loss_arr.append(np.sqrt(mean_squared_error(predictions,y_test.cpu())))\n",
    "\n",
    "    print(f'Epoch: {j} Loss: {np.mean(m_loss)} Val Loss: {np.sqrt(mean_squared_error(predictions,y_test.cpu()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25431fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
